import os
import random
import sys

import gym
import numpy as np
from gym import wrappers
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env

import torch as th

import malware_rl

TARGET = 'ember'
SEED = 26731

random.seed(SEED)
module_path = os.path.split(os.path.abspath(sys.modules[__name__].__file__))[0]
outdir = os.path.join(module_path, "data/logs/ppo-agent-results")

# Setting up the environment
env = make_vec_env(f"{TARGET}-train-v0", n_envs=1)
env.seed(SEED)

# Setting up testing parameters and holding variables
episode_count = 200
done = False
reward = 0
evasions = 0
evasion_history = {}


# Train the agent
policy_kwargs = dict(activation_fn=th.nn.Tanh,
                     net_arch=[dict(pi=[64, 64], vf=[64, 64])])

agent = PPO("MlpPolicy", 
            env, 
            gamma=0.9657974584790149,
            n_epochs=10,
            verbose=1, 
            n_steps=512,
            learning_rate=0.0001355978892506237,
            max_grad_norm=0.33249515092054016, 
            tensorboard_log=f"./ppo_{TARGET}_tensorboard/", 
            policy_kwargs=policy_kwargs) 
            # device='cpu')

# Total timesteps should be a multiple of envs*n_steps 
agent.learn(total_timesteps=4096)
agent.save(f"ppo-only-{TARGET}-train-v0-{SEED}")

print("[*] Evaluation phase begins")
eval_env = gym.make(f"{TARGET}-test-v0")
eval_env = wrappers.Monitor(eval_env, directory=outdir, force=True)

eval_env.seed(0)

# Test the agent in the eval environment
for i in range(episode_count):
    ob = eval_env.reset()
    sha256 = eval_env.sha256
    while True:
        action, _ = agent.predict(ob, reward, done)
        ob, reward, done, ep_history = eval_env.step(action)
        if done and reward >= 10.0:
            evasions += 1
            evasion_history[sha256] = ep_history
            break

        elif done:
            break

# Remove the skipped binaries
total_detected = episode_count - eval_env.skipped
# Output metrics/evaluation stuff
evasion_rate = (evasions / total_detected) * 100
mean_action_count = np.mean(eval_env.get_episode_lengths())
print(f"{evasion_rate}% samples evaded model.")
print(f"Average of {mean_action_count} moves to evade model.")
print("History:", evasion_history)
