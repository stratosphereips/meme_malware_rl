import os
import random
import sys
import logging

import gym
from gym.envs.registration import register

import numpy as np
from gym import wrappers
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import torch as th

from surrogate import train_surrogate

import malware_rl

logging.basicConfig(filename="training.log",
                    filemode='a',
                    format='%(asctime)s,%(msecs)d %(levelname)s %(message)s',
                    datefmt='%H:%M:%S',
                    level=logging.DEBUG)


TARGET = 'ember'
SEED = 26731
num_boosting_rounds = 1000
init_timesteps = 2048
num_timesteps = 4096
eval_timesteps = 2048

total_queries = 0

random.seed(SEED)
module_path = os.path.split(os.path.abspath(sys.modules[__name__].__file__))[0]
outdir = os.path.join(module_path, "data/logs/ppo-agent-results")
save_model_path = os.path.join(module_path, "malware_rl/envs/utils")
data_path = os.path.join(module_path, f"data/memory/{TARGET}")

def init_clean():
    # Delete data from previous experiments
    if os.path.exists(os.path.join(save_model_path, 'observations.npy')):
        os.remove(os.path.join(save_model_path, 'observations.npy'))
    if os.path.exists(os.path.join(save_model_path, 'scores.npy')):
        os.remove(os.path.join(save_model_path, 'scores.npy'))
    if os.path.exists(os.path.join(save_model_path, 'lgb_model.txt')):
        os.remove(os.path.join(save_model_path, 'lgb_model.txt'))

    # Delete the memory files too, even though they will be overwritten
    if os.path.exists(os.path.join(data_path, 'observations.npy')):
        os.remove(os.path.join(data_path, 'observations.npy'))
    if os.path.exists(os.path.join(data_path, 'scores.npy')):
        os.remove(os.path.join(data_path, 'scores.npy'))


def register_env(env_name):
    max_turns = gym.envs.registration.registry.env_specs['ember-train-v0']._kwargs["maxturns"]
    sha256_list = gym.envs.registration.registry.env_specs['ember-train-v0']._kwargs["sha256list"]
    
    if env_name in gym.envs.registration.registry.env_specs:
        logging.debug(f"Remove {env_name} from registry")
        del gym.envs.registration.registry.env_specs[env_name]


    register(
        id=env_name,
        entry_point="malware_rl.envs:LGBEnv",
        kwargs={
            "random_sample": True,
            "maxturns": max_turns,
            "sha256list": sha256_list,
            "save_modified_data": False
        },
)

def evaluate_agent(agent, env_string, num_queries):
    num_episodes = 200
    done = False
    reward = 0
    evasions = 0
    # true_episode_count = 0
    evasion_history = {}
    
    eval_env = gym.make(env_string)
    eval_env = wrappers.Monitor(eval_env, directory=outdir, force=True)

    # Test the agent in the eval environment
    for i in range(num_episodes):
        ob = eval_env.reset()
        sha256 = eval_env.sha256
        while True:
            action, _ = agent.predict(ob, reward, done)
            ob, reward, done, ep_history = eval_env.step(action)
            if done and reward >= 10.0:
                evasions += 1
                evasion_history[sha256] = ep_history
                break
            elif done:
                break
        if eval_env.queries >= num_queries:
            break 

    logging.debug(f"True episode count in eval: {i+1}")
    logging.debug(f"Skipped binaries: {eval_env.skipped}")

    # Remove the skipped binaries
    total_detected = (i+1) - eval_env.skipped
    # Output metrics/evaluation stuff
    evasion_rate = (evasions / total_detected) * 100
    mean_action_count = np.mean(eval_env.get_episode_lengths())
    logging.info(f"{evasion_rate}% samples evaded model.")
    logging.info(f"Average of {mean_action_count} moves to evade model.")
    print("History:", evasion_history)

    return eval_env.queries


# Step 0: Init a policy and gather some data (calculate number of queries)
# First clean all the previous data and models
logging.debug("Starting experiment")
init_clean()

# Setting up the environment
env = make_vec_env(f"{TARGET}-train-v0", n_envs=1)
env.seed(SEED)

# Train the agent
policy_kwargs = dict(activation_fn=th.nn.Tanh,
                     net_arch=dict(pi=[64, 64], vf=[64, 64]))
agent_init = PPO("MlpPolicy", 
            env, 
            gamma=0.9657974584790149,
            n_epochs=10,
            verbose=1, 
            n_steps=128,
            learning_rate=0.0001355978892506237,
            max_grad_norm=0.33249515092054016, 
            tensorboard_log=f"./ppo_{TARGET}_tensorboard/", 
            policy_kwargs=policy_kwargs) 
            # device='cpu')

# Total timesteps should be a multiple of envs*n_steps 
agent_init.learn(total_timesteps=init_timesteps)
agent_init.save(f"ppo-{TARGET}-train-v0-{SEED}-init")

# keep track of the queries
total_queries += init_timesteps

for i in range(2):

    # Step 2: Train a new model (or ensemble) using the new data
    # Step 2a: evaluate model on agreement with the target
    logging.debug(f"Training the surrogate. Round: {i+1}")
    train_surrogate(data_path, save_model_path, num_boosting_rounds)

    # Step 3: use the new model as target and train a new policy
    # Setting up the environment
    register_env('lgb-train-v0')
    env = make_vec_env(f"lgb-train-v0", n_envs=1)
    env.seed(SEED)

    # Train the agent
    logging.info(f"Training the agent on the surrogate. Round: {i+1}")
    # TODO: Should we train from scratch with the new data or not?
    policy_kwargs = dict(activation_fn=th.nn.Tanh,
                     net_arch=dict(pi=[64, 64], vf=[64, 64]))
    agent = PPO("MlpPolicy", 
            env, 
            gamma=0.9657974584790149,
            n_epochs=10,
            verbose=1, 
            n_steps=128,
            learning_rate=0.0001355978892506237,
            max_grad_norm=0.33249515092054016, 
            tensorboard_log=f"./ppo_lgb_tensorboard/", 
            policy_kwargs=policy_kwargs) 
            # device='cpu')

    # These time steps are with the surrogate hence no increase in the counter
    agent.learn(total_timesteps=num_timesteps)
    agent.save(f"ppo-{TARGET}-train-v0-{SEED}")

    # Step 4: Use the policy to evaluate the agent and save the data in the process
    # Evaluate evasion rate and keep track of number of queries
    # Go back to step 2
    
    if i < 1:
        logging.info(f"Evaluation of the agent on the target env. Round: {i+1}")
        total_queries += evaluate_agent(agent, f"{TARGET}-train-v0", eval_timesteps)
    else:
        # if it's the last round just evaluate to get the results
        logging.info(f"Final eval on the test set. Round: {i+1}")
        evaluate_agent(agent, f"{TARGET}-test-v0", 3000)

logging.info(f"Total number of queries: {total_queries}")
