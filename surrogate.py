import ember 
import numpy as np
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score, hamming_loss, confusion_matrix, recall_score, precision_score, roc_auc_score, roc_curve
from sklearn.model_selection import train_test_split

import lightgbm as lgb
import os 

import logging
logger = logging.getLogger(__name__)


def get_ember_predictions(X_test):
    model = lgb.Booster(model_file='/data/mari/ember2018/ember_model_2018.txt')
    y_proba = model.predict(X_test)
    y_pred = [int(i > 0.8336) for i in y_proba]

    return y_pred


def get_ember_data(data_dir, seed=42):
    emberdf = ember.read_metadata(data_dir)
    X_train, y_train, X_test, y_test = ember.read_vectorized_features(data_dir)

    # Get the labels of the "unlabeled" data using the avclass column
    # This is only required for the dual input DNN but is done for all models
    # for reasons of uniformity.
    idx_unlab_mal = emberdf.query("label==-1")["avclass"].dropna().index
    idx_unlab_ben = emberdf[emberdf.avclass.isnull()].query("label == -1").index

    # Read the data and select the unlabeled instances
    X_train_mal = X_train[idx_unlab_mal]
    X_train_ben = X_train[idx_unlab_ben]

    y_train_ben = np.zeros(X_train_ben.shape[0])
    y_train_mal = np.ones(X_train_mal.shape[0])

    # Create the training dataset
    y_train = np.concatenate((y_train_ben, y_train_mal))
    X_train = np.vstack((X_train_ben, X_train_mal))

    X_train_unlab, y_train_unlab = shuffle(X_train, y_train, random_state=seed)

    return X_train_unlab, X_test, y_train_unlab, y_test


def get_sorel_data(data_dir, seed=42):

    ndim = 2381
    X_train_path = os.path.join(data_dir, "X_val.dat")
    y_train_path = os.path.join(data_dir, "y_val.dat")
    y_train = np.memmap(y_train_path, dtype=np.float32, mode="r")
    N = y_train.shape[0]
    X_train = np.memmap(X_train_path, dtype=np.float32, mode="r", shape=(N, ndim))

    X_test_path = os.path.join(data_dir, "X_test.dat")
    y_test_path = os.path.join(data_dir, "y_test.dat")
    y_test = np.memmap(y_test_path, dtype=np.float32, mode="r")
    N = y_test.shape[0]
    X_test = np.memmap(X_test_path, dtype=np.float32, mode="r", shape=(N, ndim))

    X_train, y_train = shuffle(X_train, y_train, random_state=seed)
    
    return X_train, X_test, y_train, y_test


def train_surrogate(target, data_path, save_model_path, seed):

    # Latest data
    logging.debug(f"Training surrogate model for target {target}.")
    X_rl = np.load(os.path.join(data_path, 'observations.npy'))
    y_rl = np.load(os.path.join(data_path, 'scores.npy'))

    # If we have prior observations append them here
    if os.path.exists(os.path.join(save_model_path, 'observations.npy')):
        X_old = np.load(os.path.join(save_model_path, 'observations.npy'))
        y_old = np.load(os.path.join(save_model_path, 'scores.npy'))

        X_rl = np.vstack((X_old, X_rl))
        y_rl = np.concatenate((y_old, y_rl))

    # Store data after merging
    np.save(os.path.join(save_model_path, 'observations.npy'), X_rl)
    np.save(os.path.join(save_model_path, 'scores.npy'), y_rl)
    logging.debug(f"Size of the RL data: {X_rl.shape[0]}")

    if target == 'ember':
        X_train, X_test, y_train, y_test = get_ember_data('/data/mari/ember2018')
    else:
        X_train, X_test, y_train, y_test = get_sorel_data('/data/mari/sorel-data')

    t = X_rl.shape[0]
    m = X_train.shape[0]

    logging.debug(f"Size of the training data: {m}")
    logging.debug(f"Size of the RL data: {t}")

    w_rl = np.ones(t) * (m/t)
    w_train = np.ones(m)

    X = np.vstack((X_train, X_rl))
    y = np.concatenate((y_train, y_rl))
    w = np.concatenate((w_train, w_rl))

    lgb_params = {
            "boosting_type" : "gbdt",
            "objective" : "binary",
            "learning_rate" : 0.05,
            "num_leaves": 1250,
            "max_depth" : 14,
            "min_child_samples": 30,
            "verbose": -1,
            "seed": seed
        }
    
    
    train_data = lgb.Dataset(X, label=y, weight=w)
    # val_data = lgb.Dataset(X_val, label=y_val, weight=w_val)

    model = lgb.train(lgb_params, train_data,
            num_boost_round=800
        ) 

    y_proba = model.predict(X_test)
    y_pred_target = get_ember_predictions(X_test)
    threshold = evaluate_surrogate(y_proba, y_pred_target, y_test, 0.01)
    
    model.save_model(os.path.join(save_model_path, f'lgb_{target}_model_{seed}.txt'))

    return threshold


def get_fpr(y_true, y_pred):
    """
        Given the true and predicted labels calculate the FPR.
        Uses the confusion_matrix() from scikit-learn.
    """
    tn, fp, _, _ = confusion_matrix(y_true, y_pred).ravel()
    fpr = fp / (tn + fp)
    return fpr


def find_threshold(y_true, y_pred, fpr_target):
    """
    Given the true labels and the probabilities of the model
    calculate the decision threshold for a given FPR level.
    """
    fpr, _, thresh = roc_curve(y_true, y_pred)
    return np.interp(fpr_target, fpr, thresh)


def evaluate_surrogate(y_proba, y_pred_target, y_test, fpr_target):
    thresh = find_threshold(y_test, y_proba, fpr_target)
    y_pred = [int(i > thresh) for i in y_proba]


    test_score = accuracy_score(y_test, y_pred)
    agg_score = 1.0 - hamming_loss(y_pred_target, y_pred)    
    fpr_score = get_fpr(y_test, y_pred)
    rec_score = recall_score(y_test, y_pred)
    pres_score = precision_score(y_test, y_pred)
    auc_score = roc_auc_score(y_test, y_pred)
    conf_mat = confusion_matrix(y_test, y_pred)

    logging.debug(f"Threshold for target FPR {fpr_target}: {thresh}")
    logging.debug(f"Accuracy score: {test_score}")
    logging.debug(f"Agreement: {agg_score}")
    logging.debug(f"FPR: {fpr_score}")
    logging.debug(f"Recall: {rec_score}")
    logging.debug(f"Precision: {pres_score}")
    logging.debug(f"AUC: {auc_score}")
    logging.debug(f"Confusion matrix: {conf_mat}")

    return thresh




